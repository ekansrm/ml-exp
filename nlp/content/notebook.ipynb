{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 先构建Content层\n",
    "- 输入变量方式\n",
    "-  单源, 双源, 多源, 靠变量变参控制\n",
    "- 是否dropout, 是否batch normalization\n",
    "\n",
    "一个比较好的形式是结合Embedding和NN, 通过Embedding 选出参数, 然后嵌入NN里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ekansrm/WORK/-.Env/DataScience-python3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras import activations\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras.engine import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.legacy import interfaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"Just your regular densely-connected NN layer.\n",
    "\n",
    "    `Dense` implements the operation:\n",
    "    `output = activation(dot(input, kernel) + bias)`\n",
    "    where `activation` is the element-wise activation function\n",
    "    passed as the `activation` argument, `kernel` is a weights matrix\n",
    "    created by the layer, and `bias` is a bias vector created by the layer\n",
    "    (only applicable if `use_bias` is `True`).\n",
    "\n",
    "    Note: if the input to the layer has a rank greater than 2, then\n",
    "    it is flattened prior to the initial dot product with `kernel`.\n",
    "\n",
    "    # Example\n",
    "\n",
    "    ```python\n",
    "        # as first layer in a sequential model:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_shape=(16,)))\n",
    "        # now the model will take as input arrays of shape (*, 16)\n",
    "        # and output arrays of shape (*, 32)\n",
    "\n",
    "        # after the first layer, you don't need to specify\n",
    "        # the size of the input anymore:\n",
    "        model.add(Dense(32))\n",
    "    ```\n",
    "\n",
    "    # Arguments\n",
    "        units: Positive integer, dimensionality of the output space.\n",
    "        activation: Activation function to use\n",
    "            (see [activations](../activations.md)).\n",
    "            If you don't specify anything, no activation is applied\n",
    "            (ie. \"linear\" activation: `a(x) = x`).\n",
    "        use_bias: Boolean, whether the layer uses a bias vector.\n",
    "        kernel_initializer: Initializer for the `kernel` weights matrix\n",
    "            (see [initializers](../initializers.md)).\n",
    "        bias_initializer: Initializer for the bias vector\n",
    "            (see [initializers](../initializers.md)).\n",
    "        kernel_regularizer: Regularizer function applied to\n",
    "            the `kernel` weights matrix\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        bias_regularizer: Regularizer function applied to the bias vector\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        activity_regularizer: Regularizer function applied to\n",
    "            the output of the layer (its \"activation\").\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        kernel_constraint: Constraint function applied to\n",
    "            the `kernel` weights matrix\n",
    "            (see [constraints](../constraints.md)).\n",
    "        bias_constraint: Constraint function applied to the bias vector\n",
    "            (see [constraints](../constraints.md)).\n",
    "\n",
    "    # Input shape\n",
    "        nD tensor with shape: `(batch_size, ..., input_dim)`.\n",
    "        The most common situation would be\n",
    "        a 2D input with shape `(batch_size, input_dim)`.\n",
    "\n",
    "    # Output shape\n",
    "        nD tensor with shape: `(batch_size, ..., units)`.\n",
    "        For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
    "        the output would have shape `(batch_size, units)`.\n",
    "    \"\"\"\n",
    "\n",
    "    @interfaces.legacy_dense_support\n",
    "    def __init__(self, units,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.input_spec = InputSpec(min_ndim=2)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "        self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = K.dot(inputs, self.kernel)\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias)\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) >= 2\n",
    "        assert input_shape[-1]\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[-1] = self.units\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "            'activation': activations.serialize(self.activation),\n",
    "            'use_bias': self.use_bias,\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
    "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': constraints.serialize(self.bias_constraint)\n",
    "        }\n",
    "        base_config = super(Dense, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(Layer):\n",
    "    \"\"\"Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "    eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "\n",
    "    This layer can only be used as the first layer in a model.\n",
    "\n",
    "    # Example\n",
    "\n",
    "    ```python\n",
    "      model = Sequential()\n",
    "      model.add(Embedding(1000, 64, input_length=10))\n",
    "      # the model will take as input an integer matrix of size (batch, input_length).\n",
    "      # the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "      # now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "      input_array = np.random.randint(1000, size=(32, 10))\n",
    "\n",
    "      model.compile('rmsprop', 'mse')\n",
    "      output_array = model.predict(input_array)\n",
    "      assert output_array.shape == (32, 10, 64)\n",
    "    ```\n",
    "\n",
    "    # Arguments\n",
    "      input_dim: int > 0. Size of the vocabulary,\n",
    "          i.e. maximum integer index + 1.\n",
    "      output_dim: int >= 0. Dimension of the dense embedding.\n",
    "      embeddings_initializer: Initializer for the `embeddings` matrix\n",
    "          (see [initializers](../initializers.md)).\n",
    "      embeddings_regularizer: Regularizer function applied to\n",
    "          the `embeddings` matrix\n",
    "          (see [regularizer](../regularizers.md)).\n",
    "      embeddings_constraint: Constraint function applied to\n",
    "          the `embeddings` matrix\n",
    "          (see [constraints](../constraints.md)).\n",
    "      mask_zero: Whether or not the input value 0 is a special \"padding\"\n",
    "          value that should be masked out.\n",
    "          This is useful when using [recurrent layers](recurrent.md)\n",
    "          which may take variable length input.\n",
    "          If this is `True` then all subsequent layers\n",
    "          in the model need to support masking or an exception will be raised.\n",
    "          If mask_zero is set to True, as a consequence, index 0 cannot be\n",
    "          used in the vocabulary (input_dim should equal size of\n",
    "          vocabulary + 1).\n",
    "      input_length: Length of input sequences, when it is constant.\n",
    "          This argument is required if you are going to connect\n",
    "          `Flatten` then `Dense` layers upstream\n",
    "          (without it, the shape of the dense outputs cannot be computed).\n",
    "\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(batch_size, sequence_length)`.\n",
    "\n",
    "    # Output shape\n",
    "        3D tensor with shape: `(batch_size, sequence_length, output_dim)`.\n",
    "\n",
    "    # References\n",
    "        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n",
    "    \"\"\"\n",
    "\n",
    "    @interfaces.legacy_embedding_support\n",
    "    def __init__(self, input_dim, output_dim,\n",
    "                 embeddings_initializer='uniform',\n",
    "                 embeddings_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 embeddings_constraint=None,\n",
    "                 mask_zero=False,\n",
    "                 input_length=None,\n",
    "                 **kwargs):\n",
    "        if 'input_shape' not in kwargs:\n",
    "            if input_length:\n",
    "                kwargs['input_shape'] = (input_length,)\n",
    "            else:\n",
    "                kwargs['input_shape'] = (None,)\n",
    "        super(Embedding, self).__init__(**kwargs)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embeddings_initializer = initializers.get(embeddings_initializer)\n",
    "        self.embeddings_regularizer = regularizers.get(embeddings_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.embeddings_constraint = constraints.get(embeddings_constraint)\n",
    "        self.mask_zero = mask_zero\n",
    "        self.input_length = input_length\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embeddings = self.add_weight(\n",
    "            shape=(self.input_dim, self.output_dim),\n",
    "            initializer=self.embeddings_initializer,\n",
    "            name='embeddings',\n",
    "            regularizer=self.embeddings_regularizer,\n",
    "            constraint=self.embeddings_constraint,\n",
    "            dtype=self.dtype)\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if not self.mask_zero:\n",
    "            return None\n",
    "        else:\n",
    "            return K.not_equal(inputs, 0)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.input_length is None:\n",
    "            return input_shape + (self.output_dim,)\n",
    "        else:\n",
    "            # input_length can be tuple if input is 3D or higher\n",
    "            if isinstance(self.input_length, (list, tuple)):\n",
    "                in_lens = list(self.input_length)\n",
    "            else:\n",
    "                in_lens = [self.input_length]\n",
    "            if len(in_lens) != len(input_shape) - 1:\n",
    "                ValueError('\"input_length\" is %s, but received input has shape %s' %\n",
    "                           (str(self.input_length), str(input_shape)))\n",
    "            else:\n",
    "                for i, (s1, s2) in enumerate(zip(in_lens, input_shape[1:])):\n",
    "                    if s1 is not None and s2 is not None and s1 != s2:\n",
    "                        ValueError('\"input_length\" is %s, but received input has shape %s' %\n",
    "                                   (str(self.input_length), str(input_shape)))\n",
    "                    elif s1 is None:\n",
    "                        in_lens[i] = s2\n",
    "            return (input_shape[0],) + tuple(in_lens) + (self.output_dim,)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if K.dtype(inputs) != 'int32':\n",
    "            inputs = K.cast(inputs, 'int32')\n",
    "        out = K.gather(self.embeddings, inputs)\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'input_dim': self.input_dim,\n",
    "                  'output_dim': self.output_dim,\n",
    "                  'embeddings_initializer': initializers.serialize(self.embeddings_initializer),\n",
    "                  'embeddings_regularizer': regularizers.serialize(self.embeddings_regularizer),\n",
    "                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
    "                  'embeddings_constraint': constraints.serialize(self.embeddings_constraint),\n",
    "                  'mask_zero': self.mask_zero,\n",
    "                  'input_length': self.input_length}\n",
    "        base_config = super(Embedding, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试数据生成\n",
    "\n",
    "定义不同的运算关系, 生产多组运算的数据, 并加入扰动\n",
    "\n",
    "问题, 高维数据运算如何压缩到1为\n",
    "\n",
    "给定维度, 给定类别\n",
    "\n",
    "1. 先生成给定唯独的类别个随机数\n",
    "2. 随机选择类别, 加上一个扰动, 生成一个在类别点附近的样本点\n",
    "3. 随机选择一个运算符, 通过这个运算符生成两个样本, 得到运算类别, 操作数1, 操作数2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 16\n",
    "category = 10\n",
    "delta = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_list = np.array([np.random.permutation(np.arange(0, 1, 1/DIM)) for i in range(0, category, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def op_add(vec):\n",
    "    op_1 = np.random.normal(loc=0., scale=delta, size=(DIM,))\n",
    "    op_2 = vec - op_1\n",
    "    return op_1, op_2\n",
    "\n",
    "\n",
    "def op_sub(vec):\n",
    "    op_1 = np.random.normal(loc=0., scale=delta, size=(DIM,))\n",
    "    op_2 = vec + op_1\n",
    "    return op_1, op_2\n",
    "\n",
    "\n",
    "def op_mul(vec):\n",
    "    op_1 = np.random.normal(loc=0., scale=delta, size=(DIM,))\n",
    "    op_2 = vec / op_1\n",
    "    return op_1, op_2\n",
    "\n",
    "\n",
    "def op_div(vec):\n",
    "    op_1 = np.random.normal(loc=0., scale=delta, size=(DIM,))\n",
    "    op_2 = vec * op_1\n",
    "    return op_1, op_2\n",
    "\n",
    "\n",
    "op_table = {\n",
    "    0: op_add,\n",
    "    1: op_sub,\n",
    "    2: op_mul,\n",
    "    3: op_div,\n",
    "}\n",
    "\n",
    "OP_CA = len(op_table)\n",
    "\n",
    "\n",
    "def gen_random():\n",
    "    category_idx = np.random.randint(low=0, high=category, size=None)\n",
    "    delta_rand = np.random.normal(loc=0., scale=delta, size=(DIM,))\n",
    "    core = cate_list[category_idx] + delta_rand\n",
    "    op_idx = np.random.randint(low=0, high=OP_CA, size=None)\n",
    "    op1, op2 = op_table[op_idx](core)\n",
    "    return op_idx, op1, op2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, array([ 0.0001944 , -0.00127819,  0.00052428, -0.0010449 ,  0.00073064,\n       -0.00068549,  0.00103429, -0.00015337, -0.00116016, -0.00107941,\n       -0.00073093, -0.00047885, -0.0005552 ,  0.00086923,  0.00039259,\n       -0.00028498]), array([ 0.93959499, -0.00271277,  0.1883002 ,  0.56313965,  0.37432171,\n        0.81035771,  0.25240696,  0.62522192,  0.68457749,  0.43997419,\n        0.49630843,  0.75035468,  0.06259316,  0.87603442,  0.31289866,\n        0.12441976]))\n"
     ]
    }
   ],
   "source": [
    "print(gen_random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_idx, op1, op2 = gen_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.89152188e-03 -4.25505420e-04 -9.87677272e-04  4.89570919e-04\n -5.97991456e-04 -5.23252691e-04 -4.02180295e-04  3.51372804e-04\n -1.02586336e-03 -1.05852849e-03 -3.17944911e-04  1.76553996e-03\n -8.40683908e-04  8.33377620e-04 -9.99113424e-04 -4.92523010e-04\n  9.39963349e-01 -4.72281142e-04  1.88187150e-01  5.62584436e-01\n  3.76788128e-01  8.13691336e-01  2.51456943e-01  6.25751113e-01\n  6.86720472e-01  4.40010405e-01  5.02374278e-01  7.48492246e-01\n  6.27927275e-02  8.73010057e-01  3.14145292e-01  1.24895472e-01]\n"
     ]
    }
   ],
   "source": [
    "print(np.append(op1, op2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
