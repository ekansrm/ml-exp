{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = 'D:\\Work\\Project\\Project.DataScience\\model\\glove\\glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 思路\n",
    "1.从glove文件生成一个word和偏移量的索引\n",
    "2.根据任务生成一个vocab\n",
    "3.根据vocab生成tokenizer和embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成测试用的文件\n",
    "TEST_EMBEDDING_FILE = 'D:\\Work\\Project\\Project.DataScience\\model\\glove\\embedding.txt'\n",
    "NUM_LINE = 100\n",
    "lines = []\n",
    "with open(glove_path, \"r\") as fp:\n",
    "    for i in range(NUM_LINE):\n",
    "        lines.append(fp.readline())\n",
    "\n",
    "with open(TEST_EMBEDDING_FILE, 'w') as fp:\n",
    "    fp.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查token是否发生变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查参数是否发生变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0% | 0/262854 | [00:00<?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r100% | 262754/262854 | [00:00<00:00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 生成 index\n",
    "def _index(embedding):\n",
    "    index = {}\n",
    "    with open(embedding, 'r', encoding='utf-8') as fp:\n",
    "        linecnt = 0\n",
    "        word = ''\n",
    "        getword = False\n",
    "        fp.seek(0, 2)\n",
    "        filesize = fp.tell()\n",
    "        fp.seek(0, 0)\n",
    "        bar_format = \"{percentage:3.0f}% | {n_fmt}/{total_fmt} | [{elapsed}<{remaining}]\"\n",
    "        with tqdm.tqdm(total=filesize, bar_format=bar_format) as p_bar:\n",
    "            while True:\n",
    "                c = fp.read(1)\n",
    "                \n",
    "                # 如果不是最后一个字符\n",
    "                if c is not \"\":\n",
    "                    linecnt += 1\n",
    "                    \n",
    "                # word为第一个空格前的字符           \n",
    "                if getword is False:\n",
    "                    if c == \" \":\n",
    "                        getword = True\n",
    "                    else:\n",
    "                        word = word + c\n",
    "                \n",
    "                # 如果已经是最后一个字符, 或者遇到回车符\n",
    "                if c == '\\n' or c is \"\":\n",
    "                    if linecnt is not 0:\n",
    "                        index[word] = (fp.tell() - linecnt - 1, linecnt)\n",
    "                    p_bar.update(linecnt)\n",
    "                    linecnt = 0\n",
    "                    getword = False\n",
    "                    word = ''\n",
    "                \n",
    "                if c is \"\":\n",
    "                    break\n",
    "   \n",
    "    '''\n",
    "    # 下面这段代码有问题, 每一行的大小都少算一个字节\n",
    "    with open(TEST_EMBEDDING_FILE, 'r') as fp:\n",
    "        for line in fp.readlines():\n",
    "            items = line.split(\" \")\n",
    "            word = items[0]\n",
    "            index[word] = (size_cnt, len(line))\n",
    "            size_cnt = size_cnt + len(line)\n",
    "    '''\n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "index = _index(TEST_EMBEDDING_FILE)\n",
    "\n",
    "\n",
    "def _check_index(embedding, index):\n",
    "    a = {}\n",
    "    b = {}\n",
    "    with open(embedding, 'r') as fp:\n",
    "        for line in fp.readlines():\n",
    "            items = line.split(\" \")\n",
    "            a[items[0]] = line\n",
    "    with open(embedding, 'r') as fp: \n",
    "        for k in index:\n",
    "            fp.seek(index[k][0], 0)\n",
    "            b[k] = fp.read(index[k][1])\n",
    "    \n",
    "    cont = 0\n",
    "    for k in a:\n",
    "        if a[k] != b[k]:\n",
    "            print(\"wrong: \" + k)\n",
    "            print(a[k])\n",
    "            print(b[k])\n",
    "        else:\n",
    "            cont += 1\n",
    "\n",
    "            print(\"correct: \" + k)\n",
    "    print(cont)\n",
    "    \n",
    "    \n",
    "# _check_index(TEST_EMBEDDING_FILE, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据vocab生成tokenizer 和 embedding\n",
    "\n",
    "def build(vocab: list):\n",
    "    # 检查是否已经存在embedding cache file, 如果没有\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
