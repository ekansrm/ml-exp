{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = 'D:\\Work\\Project\\Project.DataScience\\model\\glove\\glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = '/mnt/DATA/glove.6B/glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 思路\n",
    "1.从glove文件生成一个word和偏移量的索引\n",
    "2.根据任务生成一个vocab\n",
    "3.根据vocab生成tokenizer和embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_EMBEDDING_FILE = 'D:\\Work\\Project\\Project.DataScience\\model\\glove\\embedding.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_EMBEDDING_FILE = 'embedding.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成测试用的文件\n",
    "NUM_LINE = 100\n",
    "lines = []\n",
    "with open(glove_path, \"r\") as fp:\n",
    "    for i in range(NUM_LINE):\n",
    "        lines.append(fp.readline())\n",
    "\n",
    "with open(TEST_EMBEDDING_FILE, 'w') as fp:\n",
    "    fp.write(''.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存文件\n",
    "import pickle\n",
    "\n",
    "\n",
    "def _save_var(var, filepath):\n",
    "    with open(file=filepath, mode=\"wb\") as fp:\n",
    "        pickle.dump(var, file=fp)\n",
    "    \n",
    "    \n",
    "def _load_var(filepath):\n",
    "    with open(file=filepath, mode=\"rb\") as fp:\n",
    "        return pickle.load(fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0% | 0/260340 | [00:00<?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r100% | 260340/260340 | [00:00<00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 生成 index\n",
    "def _index(embedding):\n",
    "    index = {}\n",
    "    with open(embedding, 'r', encoding='utf-8') as fp:\n",
    "        linecnt = 0\n",
    "        word = ''\n",
    "        getword = False\n",
    "        fp.seek(0, 2)\n",
    "        filesize = fp.tell()\n",
    "        fp.seek(0, 0)\n",
    "        bar_format = \"{percentage:3.0f}% | {n_fmt}/{total_fmt} | [{elapsed}<{remaining}]\"\n",
    "        with tqdm.tqdm(total=filesize, bar_format=bar_format) as p_bar:\n",
    "            while True:\n",
    "                c = fp.read(1)\n",
    "                \n",
    "                # 如果不是最后一个字符\n",
    "                if c is not \"\":\n",
    "                    linecnt += 1\n",
    "                    \n",
    "                # word为第一个空格前的字符           \n",
    "                if getword is False:\n",
    "                    if c == \" \":\n",
    "                        getword = True\n",
    "                    else:\n",
    "                        word = word + c\n",
    "                \n",
    "                # 如果已经是最后一个字符, 或者遇到回车符\n",
    "                if c == '\\n' or c is \"\":\n",
    "                    if linecnt is not 0:\n",
    "                        index[word] = (fp.tell() - linecnt, linecnt)\n",
    "                    p_bar.update(linecnt)\n",
    "                    linecnt = 0\n",
    "                    getword = False\n",
    "                    word = ''\n",
    "                \n",
    "                if c is \"\":\n",
    "                    break\n",
    "   \n",
    "    '''\n",
    "    # 下面这段代码有问题, 每一行的大小都少算一个字节\n",
    "    with open(TEST_EMBEDDING_FILE, 'r') as fp:\n",
    "        for line in fp.readlines():\n",
    "            items = line.split(\" \")\n",
    "            word = items[0]\n",
    "            index[word] = (size_cnt, len(line))\n",
    "            size_cnt = size_cnt + len(line)\n",
    "    '''\n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "index = _index(TEST_EMBEDDING_FILE)\n",
    "\n",
    "\n",
    "def _check_index(embedding, index):\n",
    "    a = {}\n",
    "    b = {}\n",
    "    with open(embedding, 'r') as fp:\n",
    "        for line in fp.readlines():\n",
    "            items = line.split(\" \")\n",
    "            a[items[0]] = line\n",
    "    with open(embedding, 'r') as fp: \n",
    "        for k in index:\n",
    "            fp.seek(index[k][0], 0)\n",
    "            b[k] = fp.read(index[k][1])\n",
    "    \n",
    "    cont = 0\n",
    "    for k in a:\n",
    "        if a[k] != b[k]:\n",
    "            print(\"wrong: \" + k)\n",
    "            print(a[k])\n",
    "            print(b[k])\n",
    "        else:\n",
    "            cont += 1\n",
    "\n",
    "    print(cont)\n",
    "    \n",
    "    \n",
    "_check_index(TEST_EMBEDDING_FILE, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "_save_var(index, \"123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_l = _load_var(\"123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(index_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = TEST_EMBEDDING_FILE\n",
    "\n",
    "def load_index():\n",
    "    index_file = embedding + \".index\"\n",
    "    return _load_var(index_file)\n",
    "\n",
    "def save_index(index):\n",
    "    index_file = embedding + \".index\"\n",
    "    _save_var(index, index_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_2 = load_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct 100, wrong 0, total 100\n"
     ]
    }
   ],
   "source": [
    "wrong_cnt = 0\n",
    "correct_cnt = 0\n",
    "total = 0\n",
    "for key in index:\n",
    "    if index[key] != index[key]:\n",
    "        wrong_cnt += 1\n",
    "    else:\n",
    "        correct_cnt += 1\n",
    "    total += 1\n",
    "print(\"correct %d, wrong %d, total %d\" %(correct_cnt, wrong_cnt, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取vector\n",
    "m_index = index\n",
    "m_embedding_fp = open(embedding, 'r')\n",
    "\n",
    "\n",
    "def _load_vec(word):\n",
    "    pos = m_index[word]\n",
    "    m_embedding_fp.seek(pos[0])\n",
    "    line = m_embedding_fp.read(pos[1])\n",
    "    items = line.split(\" \")\n",
    "    word_in_embedding_file = items[0]\n",
    "    if word_in_embedding_file != word:\n",
    "        raise Exception(\"index not correct\")\n",
    "    return list(map(float, items[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_token = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据vocab加载embedding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _build_token(vocab):\n",
    "    for i, word in enumerate(vocab):\n",
    "        m_token[word] = i\n",
    "    return m_token\n",
    "\n",
    "def build(vocab, cache=None):\n",
    "    if cache is not None:\n",
    "        m_my_embedding = cache\n",
    "        m_my_embedding_token = cache + \".token\"\n",
    "        \n",
    "        if os.path.exists(cache):\n",
    "            my_token = _load_var(m_my_embedding_token)\n",
    "            new_word = set(vocab) - set(my_token.keys())\n",
    "            print(my_token)\n",
    "            print(new_word)\n",
    "\n",
    "        else:\n",
    "            new_word = set(vocab)\n",
    "        \n",
    "        if len(new_word) > 0:\n",
    "            embedding_vec = list([_load_vec(word) for word in vocab])\n",
    "            m_token = _build_token(vocab)\n",
    "            _save_var(m_token, m_my_embedding_token)\n",
    "            _save_var(embedding_vec, m_my_embedding)\n",
    "            \n",
    "        else:\n",
    "            embedding_vec = _load_var(m_my_embedding)\n",
    "            \n",
    "    else:\n",
    "        embedding_vec = list([_load_vec(word) for word in vocab])\n",
    "                    \n",
    "    return embedding_vec\n",
    "\n",
    "\n",
    "def tokenize(word):\n",
    "    return m_token[word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['of', 'to', 'her']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'of': 0, 'to': 1, 'and': 2, 'you': 3, 'her': 2}\nset()\n"
     ]
    }
   ],
   "source": [
    "vec = build(vocab, 'test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n{}\n"
     ]
    }
   ],
   "source": [
    "print(len(vec))\n",
    "print(m_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'of'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-0bba33e26497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'of'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-171-8fa30ac66a2c>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mm_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'of'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "print(tokenize('of'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
